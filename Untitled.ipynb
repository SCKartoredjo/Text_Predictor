{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Recurrent Neural Network\n",
    "\n",
    "Tutorial/example [here](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 6} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16068241615971367872\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unique chars: \t 43\n",
      "Total chars: \t 144342\n"
     ]
    }
   ],
   "source": [
    "print('#unique chars: \\t', len(chars))\n",
    "print('Total chars: \\t', len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144242\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example dataX (len = 100 ):\n",
      "\n",
      " [19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31, 30, 1, 36] \n",
      "\n",
      "['c', 'h', 'a', 'p', 't', 'e', 'r', ' ', 'i', '.', ' ', 'd', 'o', 'w', 'n', ' ', 't', 'h', 'e', ' ', 'r', 'a', 'b', 'b', 'i', 't', '-', 'h', 'o', 'l', 'e', '\\n', '\\n', 'a', 'l', 'i', 'c', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'v', 'e', 'r', 'y', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'o', 'f', ' ', 's', 'i', 't', 't', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'h', 'e', 'r', ' ', 's', 'i', 's', 't', 'e', 'r', ' ', 'o', 'n', ' ', 't']\n"
     ]
    }
   ],
   "source": [
    "print('Example dataX (len =', len(dataX[1]), '):\\n\\n', dataX[0], '\\n')\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "seq_in = [int_to_char[value] for value in dataX[0]]\n",
    "\n",
    "print(seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0310 00:21:44.616253 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0310 00:21:44.618247 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0310 00:21:44.620628 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0310 00:21:44.828101 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0310 00:21:44.832063 14496 deprecation.py:506] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0310 00:21:44.846049 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0310 00:21:44.858998 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"models\\weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 00:27:28.445236 14496 deprecation.py:323] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0310 00:27:28.777085 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0310 00:27:28.822002 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144242/144242 [==============================] - 228s 2ms/step - loss: 2.7574\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.75737, saving model to models\\weights-improvement-01-2.7574.hdf5\n",
      "Epoch 2/20\n",
      "144242/144242 [==============================] - 230s 2ms/step - loss: 2.6455\n",
      "\n",
      "Epoch 00002: loss improved from 2.75737 to 2.64547, saving model to models\\weights-improvement-02-2.6455.hdf5\n",
      "Epoch 3/20\n",
      "144242/144242 [==============================] - 228s 2ms/step - loss: 2.5656\n",
      "\n",
      "Epoch 00003: loss improved from 2.64547 to 2.56558, saving model to models\\weights-improvement-03-2.5656.hdf5\n",
      "Epoch 4/20\n",
      "144242/144242 [==============================] - 232s 2ms/step - loss: 2.4968\n",
      "\n",
      "Epoch 00004: loss improved from 2.56558 to 2.49676, saving model to models\\weights-improvement-04-2.4968.hdf5\n",
      "Epoch 5/20\n",
      "144242/144242 [==============================] - 243s 2ms/step - loss: 2.4363\n",
      "\n",
      "Epoch 00005: loss improved from 2.49676 to 2.43634, saving model to models\\weights-improvement-05-2.4363.hdf5\n",
      "Epoch 6/20\n",
      "144242/144242 [==============================] - 242s 2ms/step - loss: 2.3792\n",
      "\n",
      "Epoch 00006: loss improved from 2.43634 to 2.37920, saving model to models\\weights-improvement-06-2.3792.hdf5\n",
      "Epoch 7/20\n",
      "144242/144242 [==============================] - 240s 2ms/step - loss: 2.3281\n",
      "\n",
      "Epoch 00007: loss improved from 2.37920 to 2.32808, saving model to models\\weights-improvement-07-2.3281.hdf5\n",
      "Epoch 8/20\n",
      "144242/144242 [==============================] - 241s 2ms/step - loss: 2.2815\n",
      "\n",
      "Epoch 00008: loss improved from 2.32808 to 2.28152, saving model to models\\weights-improvement-08-2.2815.hdf5\n",
      "Epoch 9/20\n",
      "144242/144242 [==============================] - 239s 2ms/step - loss: 2.2354\n",
      "\n",
      "Epoch 00009: loss improved from 2.28152 to 2.23536, saving model to models\\weights-improvement-09-2.2354.hdf5\n",
      "Epoch 10/20\n",
      "144242/144242 [==============================] - 234s 2ms/step - loss: 2.1905\n",
      "\n",
      "Epoch 00010: loss improved from 2.23536 to 2.19045, saving model to models\\weights-improvement-10-2.1905.hdf5\n",
      "Epoch 11/20\n",
      "144242/144242 [==============================] - 228s 2ms/step - loss: 2.1490\n",
      "\n",
      "Epoch 00011: loss improved from 2.19045 to 2.14901, saving model to models\\weights-improvement-11-2.1490.hdf5\n",
      "Epoch 12/20\n",
      "144242/144242 [==============================] - 236s 2ms/step - loss: 2.1099\n",
      "\n",
      "Epoch 00012: loss improved from 2.14901 to 2.10986, saving model to models\\weights-improvement-12-2.1099.hdf5\n",
      "Epoch 13/20\n",
      "144242/144242 [==============================] - 242s 2ms/step - loss: 2.0720\n",
      "\n",
      "Epoch 00013: loss improved from 2.10986 to 2.07198, saving model to models\\weights-improvement-13-2.0720.hdf5\n",
      "Epoch 14/20\n",
      "144242/144242 [==============================] - 248s 2ms/step - loss: 2.0398\n",
      "\n",
      "Epoch 00014: loss improved from 2.07198 to 2.03982, saving model to models\\weights-improvement-14-2.0398.hdf5\n",
      "Epoch 15/20\n",
      "144242/144242 [==============================] - 249s 2ms/step - loss: 2.0075\n",
      "\n",
      "Epoch 00015: loss improved from 2.03982 to 2.00750, saving model to models\\weights-improvement-15-2.0075.hdf5\n",
      "Epoch 16/20\n",
      "144242/144242 [==============================] - 250s 2ms/step - loss: 1.9738\n",
      "\n",
      "Epoch 00016: loss improved from 2.00750 to 1.97377, saving model to models\\weights-improvement-16-1.9738.hdf5\n",
      "Epoch 17/20\n",
      "144242/144242 [==============================] - 250s 2ms/step - loss: 1.9455\n",
      "\n",
      "Epoch 00017: loss improved from 1.97377 to 1.94546, saving model to models\\weights-improvement-17-1.9455.hdf5\n",
      "Epoch 18/20\n",
      " 58368/144242 [===========>..................] - ETA: 2:32 - loss: 1.9097"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 00:21:51.153665 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0310 00:21:51.154625 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0310 00:21:51.154625 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0310 00:21:51.177557 14496 module_wrapper.py:139] From C:\\Users\\Svenja\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"models\\weights-improvement-01-2.9688.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" plied.\n",
      " \"there is another shore, you know, upon the other side.\n",
      " the further off from england the ne \"\n",
      "e the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
